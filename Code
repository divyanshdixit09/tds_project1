#Scraping :
# Install necessary libraries
!pip install requests pandas

# Import required libraries
import requests
import pandas as pd
import os

# Set your GitHub access token here
GITHUB_TOKEN = "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"  # Add your GitHub personal access token
HEADERS = {"Authorization": f"token {GITHUB_TOKEN}"}

# Helper function to fetch GitHub data
def fetch_github_data(url, params={}):
    response = requests.get(url, headers=HEADERS, params=params)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error fetching data from {url}: {response.status_code}")
        return None

# Function to scrape GitHub users from Tokyo with over 200 followers, limiting to 200 users
def get_users_from_tokyo(min_followers=200, max_users=546):
    users = []
    url = "https://api.github.com/search/users"
    params = {
        "q": f"location:Tokyo followers:>{min_followers}",
        "per_page": 100,
        "page": 1
    }

    while len(users) < max_users:
        result = fetch_github_data(url, params)
        if result and 'items' in result:
            users.extend(result['items'])
            if len(result['items']) < 100 or len(users) >= max_users:
                break
            params['page'] += 1
        else:
            break
    return users[:max_users]  # Ensure we return only up to the max_users limit

# Function to clean up company names
def clean_company_name(company):
    if company:
        company = company.strip().lstrip('@').upper()
    return company if company else None

# Function to get user details
def get_user_details(user_login):
    url = f"https://api.github.com/users/{user_login}"
    user_data = fetch_github_data(url)
    return user_data

# Function to scrape repositories of a user, limiting to 500 repositories
def get_user_repositories(user_login, max_repos=500):
    repos = []
    url = f"https://api.github.com/users/{user_login}/repos"
    params = {"per_page": 100, "page": 1, "sort": "pushed"}

    while len(repos) < max_repos:
        result = fetch_github_data(url, params)
        if result:
            repos.extend(result)
            if len(result) < 100 or len(repos) >= max_repos:
                break
            params['page'] += 1
        else:
            break
    return repos[:max_repos]  # Ensure we return only up to the max_repos limit

# Main function to collect data and save it to CSV
def collect_data():
    users_data = []
    repos_data = []

    # Get users from Tokyo with more than 200 followers, limited to 200 users
    users = get_users_from_tokyo()

    for user in users:
        user_details = get_user_details(user['login'])
        if user_details:
            # Collect user details in the required format
            user_info = {
                'login': user_details.get('login'),
                'name': user_details.get('name'),
                'company': clean_company_name(user_details.get('company')),
                'location': user_details.get('location'),
                'email': user_details.get('email'),
                'hireable': str(user_details.get('hireable', 'false')).lower(),  # Ensure boolean as 'true'/'false'
                'bio': user_details.get('bio'),
                'public_repos': user_details.get('public_repos'),
                'followers': user_details.get('followers'),
                'following': user_details.get('following'),
                'created_at': user_details.get('created_at'),
            }
            users_data.append(user_info)

            # Get repositories for each user, limited to 500 repositories
            repos = get_user_repositories(user['login'], max_repos=500)
            for repo in repos:
                # Collect repository details in the required format
                repo_info = {
                    'login': user['login'],
                    'full_name': repo.get('full_name'),
                    'created_at': repo.get('created_at'),
                    'stargazers_count': repo.get('stargazers_count'),
                    'watchers_count': repo.get('watchers_count'),
                    'language': repo.get('language'),
                    'has_projects': str(repo.get('has_projects', 'false')).lower(),  # Ensure boolean as 'true'/'false'
                    'has_wiki': str(repo.get('has_wiki', 'false')).lower(),  # Ensure boolean as 'true'/'false'
                    'license_name': repo['license']['key'] if repo.get('license') else None,
                }
                repos_data.append(repo_info)

    # Convert to DataFrames and save to CSV
    users_df = pd.DataFrame(users_data)
    repos_df = pd.DataFrame(repos_data)

    users_df.to_csv('users.csv', index=False)
    repos_df.to_csv('repositories.csv', index=False)

    print("Data collection complete! CSV files saved.")

# Create README.md file
def create_readme():
    content = """
    # GitHub Data Scraper for Tokyo-based Developers

    This script scrapes GitHub users located in Tokyo with more than 200 followers using the GitHub API.

    ## Key Features:
    - **Users**: For each user, the script retrieves details such as their GitHub login, name, company, location, email, and more.
    - **Repositories**: For each user, the script fetches up to the 500 most recently pushed repositories with details such as programming language, stars, watchers, and licenses.

    ## Output Files:
    - **users.csv**: Contains user information (login, name, company, location, etc.).
    - **repositories.csv**: Contains repository information (login, repo name, language, stars, etc.).

    ## Usage:
    To collect GitHub user data from Tokyo with over 200 followers, simply run the script.
    """
    with open('README.md', 'w') as f:
        f.write(content.strip())

# Run the data collection and README creation
collect_data()
create_readme()

# Download the CSV files for further use
from google.colab import files
files.download('users.csv')
files.download('repositories.csv')
files.download('README.md')
#______________________________________________________________________________________________________________________________________________________________________________________________________________________

#Loading the data set :

import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression

users_df = pd.read_csv('users.csv')
repos_df = pd.read_csv('repositories.csv')

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

#QUES1
top_5_users_by_followers = users_df.sort_values(by='followers', ascending=False)['login'].head(5).tolist()
print(f"1. Top 5 users with highest followers: {', '.join(top_5_users_by_followers)}")
#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 2: Who are the 5 earliest registered GitHub users in Tokyo?
earliest_5_users = users_df.sort_values(by='created_at')['login'].head(5).tolist()
print(f"2. Earliest 5 users: {', '.join(earliest_5_users)}")
#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 3: What are the 3 most popular licenses among these users?
top_3_licenses = repos_df['license_name'].dropna().value_counts().head(3).index.tolist()
print(f"3. Top 3 most popular licenses: {', '.join(top_3_licenses)}")
#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 4: Which company do the majority of these developers work at?
most_common_company = users_df['company'].mode().iloc[0]
print(f"4. Majority work at: {most_common_company}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 5: Which programming language is most popular among these users?
most_popular_language = repos_df['language'].value_counts().idxmax()
print(f"5. Most popular language: {most_popular_language}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 6: Which programming language is the second most popular among users who joined after 2020?
users_post_2020 = users_df[pd.to_datetime(users_df['created_at']) > '2020-01-01']
repos_post_2020 = repos_df[repos_df['login'].isin(users_post_2020['login'])]
second_popular_language_post_2020 = repos_post_2020['language'].value_counts().index[1]
print(f"6. Second most popular language post-2020: {second_popular_language_post_2020}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________


# Question 7: Which language has the highest average number of stars per repository?
avg_stars_per_language = repos_df.groupby('language')['stargazers_count'].mean().idxmax()
print(f"7. Language with highest avg stars: {avg_stars_per_language}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 8: Top 5 in terms of leader_strength (followers / (1 + following))
users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])
top_5_leader_strength = users_df.sort_values(by='leader_strength', ascending=False)['login'].head(5).tolist()
print(f"8. Top 5 users by leader_strength: {', '.join(top_5_leader_strength)}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 9: Correlation between followers and public repositories
corr_followers_repos, _ = pearsonr(users_df['followers'], users_df['public_repos'])
print(f"9. Correlation between followers and public repos: {corr_followers_repos:.3f}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 10: Regression slope of followers on public repositories
X_repos = users_df['public_repos'].values.reshape(-1, 1)
y_followers = users_df['followers'].values
reg_repos_followers = LinearRegression().fit(X_repos, y_followers)
slope_repos_followers = reg_repos_followers.coef_[0]
print(f"10. Regression slope (followers on repos): {slope_repos_followers:.3f}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 11: Correlation between having projects enabled and wiki enabled
projects_wiki_corr, _ = pearsonr(repos_df['has_projects'], repos_df['has_wiki'])
print(f"11. Correlation between projects and wiki enabled: {projects_wiki_corr:.3f}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 12 :
# Convert 'true'/'false' strings to proper boolean values (True/False)
users_df['hireable'] = users_df['hireable'].map({'true': True, 'none': False})
# Calculate the average following for hireable and non-hireable users
avg_following_hireable = users_df[users_df['hireable'] == True]['following'].mean()
avg_following_non_hireable = users_df[users_df['hireable'] == False]['following'].mean()
# Calculate the difference in averages
difference_following = avg_following_hireable - avg_following_non_hireable
# Print the result to 3 decimal places
print(f"12. Difference in avg following (hireable vs non-hireable): {avg_following_hireable:.3f}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 13: Correlation of bio length with followers (ignore users without bios)
users_with_bio = users_df.dropna(subset=['bio'])
# Calculate the length of each user's bio in Unicode characters
users_with_bio['bio_length'] = users_with_bio['bio'].apply(len)
# Features (bio length) and target (followers)
X_bio_length = users_with_bio['bio_length'].values.reshape(-1, 1)
y_followers_bio = users_with_bio['followers'].values
# Perform linear regression
reg_bio_followers = LinearRegression().fit(X_bio_length, y_followers_bio)
# Get the regression slope (coefficient for bio length)
slope_bio_followers = reg_bio_followers.coef_[0]
# Output the slope rounded to 3 decimal places
print(f"13. Regression slope (followers on bio length): {slope_bio_followers:.3f}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Question 14: Who created the most repositories on weekends (UTC)?
repos_df['created_at'] = pd.to_datetime(repos_df['created_at'])
repos_df['day_of_week'] = repos_df['created_at'].dt.dayofweek
weekend_repos = repos_df[repos_df['day_of_week'] >= 5]  # 5 = Saturday, 6 = Sunday
top_5_weekend_creators = weekend_repos['login'].value_counts().head(5).index.tolist()
print(f"14. Top 5 users creating repos on weekends: {', '.join(top_5_weekend_creators)}")

#________________________________________________________________________________________________________________________________________________________________________________________________________________________

#Question 15 : 
# Convert 'hireable' column values to boolean, if not already
users_df['hireable'] = users_df['hireable'].astype(str).str.lower().map({'true': True, 'none': False})
# Calculate the fraction of hireable users with emails
hireable_users = users_df[users_df['hireable'] == True]
fraction_hireable_with_email = hireable_users['email'].notna().mean()
# Calculate the fraction of non-hireable users with emails
non_hireable_users = users_df[users_df['hireable'] == False]
fraction_non_hireable_with_email = non_hireable_users['email'].notna().mean()
# Calculate the difference between the two fractions
email_fraction_difference = fraction_hireable_with_email - fraction_non_hireable_with_email
# Output the result rounded to 3 decimal places
print(f"15. Fraction difference (hireable vs non-hireable with email): {email_fraction_difference:.3f}")
#________________________________________________________________________________________________________________________________________________________________________________________________________________________
#Question 16 :
from collections import Counter
# Drop users without a name and trim any extra whitespace in the 'name' column
users_with_names = users_df.dropna(subset=['name'])
users_with_names['name'] = users_with_names['name'].str.strip()
# Extract the last word (surname) from the 'name' column
users_with_names['surname'] = users_with_names['name'].apply(lambda x: x.split()[-1])
# Count the occurrences of each surname
surname_counts = Counter(users_with_names['surname'])
# Find the maximum count
max_count = max(surname_counts.values())
# Find all surnames that have the maximum count
most_common_surnames = [surname for surname, count in surname_counts.items() if count == max_count]
# Sort the most common surnames alphabetically
most_common_surnames.sort()
# Output the most common surname(s) and the count of users with that surname
print(f"16. Most common surname(s): {', '.join(most_common_surnames)}")
print(f"Number of users with the most common surname: {max_count}")


